[
  {
    "path": "posts/2021-07-25-bromans-socks/",
    "title": "A Fully Bayesian Analysis of Broman's Socks",
    "description": "When Karl Broman tweeted about his laundry he likely didn't imagine that people would still be estimating how many socks he washed 7 years later. In this post my willingness to derive some exact formulae will enable a fully Bayesian, sampling free, approach to laundry quantification.",
    "author": [],
    "date": "2021-07-25",
    "categories": [
      "Bayesian",
      "Discrete Probability"
    ],
    "contents": "\n\nContents\nAssumptions\nBååth’s Model\nFrom Prior to Posterior\nAcknowledgments\nComputational Considerations\nBååth’s Prior\nUtility Functions\n\n\n\n\n\nThat the 1st 11 socks in the laundry are each distinct suggests there are a lot more socks. pic.twitter.com/EGSo9P6rw7— Karl Broman (@kwbroman) October 17, 2014\n\n\nI belatedly found my way to the puzzle of estimating exactly how many socks Karl Broman washed through Rasmus Bååth’s excellent blog post, which uses the problem to illustrate Approximate Bayesian Computation.\nRasmus wraps-up the post by presenting three potential criticisms of his analysis, of which one is Why use approximate Bayesian computation at all? I’ll take up his challenge and derive an explicit formula for the likelihood function, enabling a Bayesian analysis without the need for sampling methods.\nI’ll also propose an alternative model to take into account my personal belief that the Tweet was sent in the knowledge that the twelfth sock was going to break the run of distinct socks!\nAssumptions\nThis post makes some assumptions about you: that you have some experience with (or a willingness to learn) statistics and discrete probability.\nI’ll assume that you’re familiar twith the concepts of Bayesian statistics: a model (the likelihood) describes our understanding of how some data is generated, it can be combined with initial assumptions about plausible parameter values (prior distributions) and combining these with observed data leads to refined assumptions (posterior distributions).\nTo follow the derivation of the likelihood you’ll need to know some common constructs from discrete probability/combinatorics: e.g. understanding of binomial coefficients, and how these relate to counting problems.\nBååth’s Model\nOur aim is to estimate the total number of socks that Karl Broman washed given his Tweet that the first 11 removed from the washing machine were distinct.\nI’ll use the following notation throughout:\n\nI use the convention that data is denoted by letters from the Roman alphabet, whilst unknown parameters use the Greek alphabet.\n\\[\n\\begin{align*} \\bf{\\text{Data}}\\\\\nd & = \\text{No. successive distinct socks observed before Tweeting}\\\\\n\\\\\n\\bf{\\text{Parameters}}\\\\\n\\rho & = \\text{No. pairs of socks in the wash}\\\\\n\\sigma & = \\text{No. singleton socks in the wash} \\\\\n\\end{align*}\n\\]\nFor the specific case of the Tweet we have \\(d = 11\\).\nUsing two parameters \\(\\rho\\) and \\(\\sigma\\) allows us to handle the scenario that whilst most socks come in pairs, in some households stray singleton socks are not uncommon. We do however assume socks don’t come in multiples of more than two (excluding the not-uncommon scenario in which two pairs of identical socks are washed).\nThe likelihood, \\(L(d|\\rho,\\sigma)\\), describes how the unknown parameters generate the obsered data: Assuming there were \\(\\rho\\) pairs of socks and \\(\\sigma\\) singletons how likely is it that the first \\(d\\) socks are all distinct?\nAs a warm up for deriving a complete formula, let’s consider some of the edge cases which have fun logical heuristics:\nScenario\nHeuristic\nLikelihood \\(L(d|\\rho,\\sigma)\\)\n\\(\\rho, \\, \\sigma < 0\\)\nLet’s not be silly: you can’t have negative socks.\n0\n\\(d > 2\\rho + \\sigma\\)\nWe can’t observe more socks than were washed.\n0\n\\(d > \\rho + \\sigma\\)\nWe can’t observe more distinct socks than the number of distinct socks that were washed.\n0\n\\(\\rho = 0, \\, \\sigma \\geq d\\)\nIf all the socks were different, then of course all the observed socks are different.\n1\nWith the edge cases handled, let’s press on and handle the substantive problem.\n\n\nLikelihood \\[\nL(\\rho,\\sigma|d) = \n\\binom{2\\rho + \\sigma}{d}^{-1} \\sum_{j=0}^{\\sigma} 2^{d-j} \\binom{\\sigma}{j} \\binom{\\rho}{d-j}\n\\]\n\n\nExamples \\[\nL(\\rho,\\sigma|d) = \n\\binom{2\\rho + \\sigma}{d}^{-1} \\sum_{j=0}^{\\sigma} 2^{d-j} \\binom{\\sigma}{j} \\binom{\\rho}{d-j}\n\\]\nAs a soft check that the formula above is correct, let’s take a look at some specific cases.\nExample: \\(\\bf{\\rho = 1, \\, \\sigma = 1, \\,d = 2}\\).\nThis is the smallest non-trial scenario, and we can check this easily by hand. If we denote the socks by {S,P1,P2}, there are three ways to choose two of them: {S,P1}, {S,P2} and {P1,P2}. In two of the scenarios the socks are distinct, so the likelihood is 2/3.\nPlugging the parameters/data into the formula above: \\[\n\\begin{align}\nL(2,1|2) & =   \\binom{3}{2}^{-1} \\left\\{ 2^2 \\binom{1}{0}\\binom{1}{2} + 2^1 \\binom{1}{1}\\binom{1}{1}\\right\\} = 3^{-1}\\left(0 + 2\\right)  = \\frac23\n\\end{align}\n\\]\nExample: \\(\\bf{\\rho=3,\\,\\sigma=4, \\, d = 4}\\)\nWhilst this example doesn’t sound much more complex, crunching numbers directly would be pretty tedious (admittedly, tricky) as the denominator of the likelihood formula suggests there are 210 scenarios to check.\nEvaluating the formula for these parameters indicates the likelihood is 129/210 ~ 0.614.\nWe can quickly validate this by simulating drawing four socks from 3 pairs and 4 singletons, and calculating the proportion of the draws that produced distinct socks.\n\n\nset.seed(1414214)\n\nrho <- 3\nsigma <- 4\nd <- 4\n\n# vector of all the socks\nall_socks <- c(rep(paste0(\"P\",1:rho), 2), paste0(\"S\", 1:sigma))\n\n# a function to sample d socks without replacement from all_socks, and return\n# 1 if all socks are distinct, and 0 otherwise\nsample_socks <- function(all_socks, d){\n  sock_sample <- sample(x = all_socks, size = d, replace = FALSE)\n  return( 1 * (length(sock_sample) == length(unique(sock_sample))) )\n}\n\n# draw samples \ndraws <- tibble(draw = map_dbl(1:1e05, ~sample_socks(all_socks, d)))\n\n\n\n\nSummary\n\nNo. Draws\n100,000\nNo. All Distinct\n61,360\nProb. All Distinct\n0.614\n\n\n\nProof\n\\[\nL(\\rho,\\sigma|d) = \n\\binom{2\\rho + \\sigma}{d}^{-1} \\sum_{j=0}^{\\sigma} 2^{d-j} \\binom{\\sigma}{j} \\binom{\\rho}{d-j}\n\\]\nProof\nIn words, the likelihood is given by the following fraction\n\\[\n\\frac{\\text{No. ways to choose d }{\\bf{distinct}}\\text{ socks from $\\rho$ pairs and $\\sigma$ singletons.}}{\\text{No. ways to choose d socks from  $\\rho$ pairs and $\\sigma$ singletons.}} \n\\]\nStarting with the denominator, this is none other than the total number of ways to choose \\(d\\) objects without replacement from a total of \\(2 \\rho + \\sigma\\) (the factor of two is because \\(\\rho\\) pairs of socks equates to \\(2 \\rho\\) individual socks). That is given by the binomial coefficient \\(\\binom{2\\rho + \\sigma}{d}\\), and explains the leading term in the likelihood formula above.\nTurning to the numerator, I’ll break this down by conditioning on the number of singleton socks. I.e. we’ll count\n\\[\\text{No. of ways to choose $d$ distinct socks, given that $j$ of them are singletons.}\\]\nStarting with the sigletons, there are \\(\\binom{\\sigma}{j}\\) ways to choose exactly \\(j\\) of these. The remaining \\(d-j\\) socks need to come from the pairs, there are \\(\\rho\\) distinct socks that form \\(2\\rho\\) pairs, so there are \\(\\binom{\\rho}{d-j}\\) ways to choose the type of socks. But then for each of these \\(d-j\\) socks we need an additional factor of 2 as we could have chosen between two (left, and right?) socks. Bringing this together, we have:\n\\[2^{d-j}\\binom{\\rho}{d-j}\\binom{\\sigma}{j}.\\]\nThe full formula for numerator, and then the likelihood, follows by summing over the possible values of \\(j = 0,\\ldots,\\sigma\\).\nFor a similar proof, I previously posted an answer to a question on Cross Validated with a slightly different sock related problem.\n\n\nIn practice when evaluating the likelihood (and later the posterior distribution) there are some computational tricks we employ to avoid running into problems of integer overlflow; these are detailed in the end-notes.\nIn the plot below we visualise the likelihood for the case of interest, \\(d = 11\\).\n\nShow code\n# log likelihood for Baath's model\nbaath_log_likelihood <- function(rho, sigma, d){\n  \n  # it is not possible to choose more than p+s distinct socks\n  if(d > rho + sigma) return(-Inf)\n  \n  # log summation terms, for the log-sum-exp trick.\n  log_summation_terms <- purrr::map(0:min(d, sigma), function(j){\n    (d-j)*log(2) + lchoose(sigma,j) + lchoose(rho,d-j)\n  })\n  \n  log_likelihood <- -lchoose(2*rho + sigma,d) +  logSumExp(log_summation_terms)\n  \n  return(log_likelihood)\n}\n\n# compute grid of likelihood values, fix d = 11\nbaath_likelihood_grid <- calculate_sock_grid(30, 20, log_likelihood = function(rho,sigma){baath_log_likelihood(rho,sigma,d=11)})\n\n# plot likelihood grid\nplot_sock_grid(baath_likelihood_grid, var = likelihood)\n\n\n\n\n\nBååth’s Likelihood\nThe plot demonstrates why a frequentist, maximum likelihood estimate (MLE) approach to solving this problem is bound to fail: the likelihood is maximised, and equal to 1, in all the scenarios that \\(\\sigma \\geq 11\\), and \\(\\rho = 0\\).\nEven if we introduced a constraint that \\(\\rho > 0\\), this just pushes MLE to return a solution at the limits \\(\\rho,\\sigma \\rightarrow \\infty\\).\nFrom Prior to Posterior\nTo avoid getting stuck in trivial edge scenarios, I’ll introduce a prior distribution over \\((\\rho,\\sigma)\\), and conduct a Bayesian Analysis. The prior captures our beliefs about the number of socks in the washing machine, in absence of any data.\nI’ll put separate priors on the number of singleton and pairs of socks, and then add a restriction that the total socks can’t exceed a fixed (large) amount. This differs from Rasmus’ approach, but produces formulae that are easier to work with: since Rasmus was using sampling, complexity in the formulae wasn’t an issue.\n\nIt is possible to derive a formula for Rasmus’ prior - but its messy! For the really invested, see the footnotes.\nCapping the maximum number of socks will be required to get an exact value for the normalising constant that turns up in using Bayes rule. If we denote \\(p(\\rho,\\sigma)\\) for the prior distribution, \\(p(\\rho, \\sigma) |d)\\) for the posterior distribution, and \\(L(d|\\rho,\\sigma)\\) the likelihdood as before, then Bayes rule gives\n\\[p(\\rho,\\sigma | d) = Z^{-1}L(d | \\rho, \\sigma) p(\\rho,\\sigma), \\qquad \\text{where } Z = \\sum_{\\rho,\\sigma} L(d | \\rho, \\sigma) p(\\rho,\\sigma).\\] By constraining the prior to a finite range the normalising constant \\(Z\\) can be calculated without estimating tail behaviour. This capping is hardly restrictive: after all in practice washing machines can’t hold arbitrarily many socks!\nThe prior distribution on \\(\\rho\\) and \\(\\sigma\\) should be a discrete distribution, on the positive integers. Like Rasmus I’ll use Negative Binomial distributions, which are a flexible generalisation of the Poisson distribution, allowing us to separately control the mean and variance.\n\n\n\nI’ll choose parameters for the Negative Binomial distributions so that that the marginal distributions of the product prior closely match those of Rasmus’ prior, which he derived based on assumptions about the plausible amount of washing a family might produce. This leads to me using the priors:\n\nParameters were found using the Method of Moments and then rounded to give nice values\n\\[\\rho \\sim \\text{NegBinom}\\left(3 \\frac{1}{4}, \\frac{1}{5}\\right), \\qquad \\sigma \\sim \\text{NegBinom}\\left(2, \\frac{1}{3}\\right),\\]\nI’ll also cap the maximum number of socks that could plausibly have been washed at 300. Googling the average weight of a sock returns a range of estimates, but I’ll errr on the lower side and say a sock ways 50g. 300 socks would therefore come in at 15kg, which is the maximum drum size of any domestic washing machine I could find on Amazon!\nWriting out the prior explicitly gives:\n\\[p(\\rho,\\sigma) = \\textstyle \\mathbf{\\large 1}_{2\\rho + \\sigma \\leq 300} \\, \\times \\, \\binom{\\rho + 2\\frac{1}{4}}{\\rho} \\left(1 - \\frac{1}{5}\\right)^{3 \\frac{1}{4}} \\left(\\frac{1}{5}\\right)^{\\rho} \\,\\,\\times\\,\\, \\binom{\\sigma + 1}{\\sigma} \\left(1 - \\frac{1}{3}\\right)^{2} \\left(\\frac{1}{3}\\right)^{\\sigma}\\]\nThe plots/table below compare samples from the product prior and the prior Rasmus uses.\n\n\n\n\n\nMarginals\n\n\n\n\nMeasure\nPrior\n10%\n25%\n50%\n75%\n90%\nTotal socks\nBååth\n13\n19\n28\n38\n50\n\nProduct\n12\n18\n27\n39\n52\nPairs of socks\nBååth\n5\n8\n12\n17\n22\n\nProduct\n4\n7\n12\n17\n24\nSingleton socks\nBååth\n1\n2\n3\n5\n8\n\nProduct\n0\n1\n3\n6\n9\nProportion single\nBååth\n0.03\n0.06\n0.1\n0.16\n0.22\n\nProduct\n0\n0.05\n0.12\n0.23\n0.37\n\n\n\nDensity\n\n\n\n\n\n\nComparison of Bååth’s Prior and the Product Prior.\nThe marginal distributions align closely, with the exception of the proportion of singleton socks which is more dispersed for the product prior. This is visible in the (empirical) density plot where Rasmus’ prior clearly has a sharper peak, and is less dispersed in the \\(\\sigma\\)-axis.\nThe density plot also shows that the product prior is smoother over the parameter space - this is to be expected as defining Rasmus’ prior requires a rounding/floor calculation (which is itself not smooth) to separate the total socks into pairs/singletons.\nFinally we can go ahead and derive the posterior using Bayes rule. The animation below shows how the posterior distribution changes with the observation of each additional distinct sock, up to \\(d=11\\).\n\nShow code\n# define the log product prior\nlog_product_prior <- function(rho, sigma, r_rho, p_rho, r_sigma, p_sigma){\n\n  if(2*rho + sigma > 300){\n    log_prior <- -Inf\n  }\n  else{\n    # relies on the fact that variables r_rho, etc. have been defined in the global\n    # environment... This isn't really best practice.\n    log_prior <- dnbinom(rho, mu = mu_rho, size = r_rho, log = TRUE) + dnbinom(sigma, mu = mu_sigma, size = r_sigma, log = TRUE)\n  }\n  \n  return(log_prior)\n}\n\n# calculate posterior grid for each of d = 0,...,11 for animation\niterated_posterior_grid <- map(0:11, .f =function(d){\n  \n  if(d == 0){\n    posterior_grid <- calculate_sock_grid(100, 100,\n        log_likelihood = NULL,\n        log_prior = function(rho,sigma){log_product_prior(rho,sigma, r_rho, p_rho, r_sigma, p_sigma)}\n      ) %>%\n      filter(rho <= 30, sigma <= 20) %>%\n      mutate(posterior =prior) %>%\n      add_column(d = d, .before = 0)\n  } else {\n    posterior_grid <- calculate_sock_grid(100, 100,\n        log_likelihood = function(rho,sigma){baath_log_likelihood(rho,sigma,d=d)},\n        log_prior = function(rho,sigma){log_product_prior(rho,sigma, r_rho, p_rho, r_sigma, p_sigma)}\n      ) %>%\n      filter(rho <= 30, sigma <= 20) %>%\n      add_column(d = d, .before = 0)\n  }\n  \n  return(posterior_grid)\n}) %>% bind_rows()\n\n# actual posterior grid\nposterior_grid <- iterated_posterior_grid %>% filter(d==11)\n\n\n\n\n\nTotal Socks\n\n\n\n\n\n10%\n25%\n50%\n75%\n90%\nPosterior\n8\n20\n40\n60\n72\nPrior\n8\n20\n40\n60\n72\n\n\n\nPosterior Grid\n\n\n\n\n\nAcknowledgments\nThis post was inspired by Karl Broman’s tweet, and Rasmus Bååth’s analysis.\nComputation has been done using R.\nComputational Considerations\nIn practice we use a couple of tricks when computing the likelihood function: avoiding the risk of encountering integer overflow when working with large sums of binomial coefficients.\nThe first trick is probably familiar: rather than working with the likelihood, we’ll use the log-likelihood. The log-likelihood for Bååth’s model is:\n\\[l(d|\\rho,\\sigma) = -\\log \\textstyle{\\binom{2\\rho + \\sigma}{d}} + \\log \\bigg(\\textstyle \\sum_{j=0}^{\\sigma} 2^{d-j} \\binom{\\sigma}{j} \\binom{\\rho}{d-j}\\bigg)\\]\nIf we naively go ahead and calculate that sum and then take logs - then we won’t avoid the risk of overflow at all: we’d still calculate a sum of large integers, and taking the logarithm becomes an after thought.\nWhat we really want to do is take the logarithm of the terms inside the summation - which will return values well within computational comfort zone. That is we want to turn a computation of the form \\(\\log \\left( x_1 + \\cdots + x_n\\right)\\) into a calculation in \\(\\log(x_1),\\ldots, \\log(x_n)\\).\nThis is where trick number two comes in: the log-sum-exp trick.\n\n\nLog-Sum-Exp Trick Given \\(x_1,\\ldots,x_n > 0\\), let \\(l_k = \\log(x_k)\\) and \\(l^* = \\max_k l_k\\). Then\n\\[\n\\log(x_1 + \\cdots +  x_n) =  l^* + \\log \\bigg ( \\exp(l_1 - l^*) + \\cdots + \\exp(l_n - l^*)\\bigg)\n\\]\n\n\nExample Given \\(x_1,\\ldots,x_n > 0\\), let \\(l_k = \\log(x_k)\\) and \\(l^* = \\max_k l_k\\). Then\n\\[\n\\log(x_1 + \\cdots +  x_n) =  l^* + \\log \\bigg ( \\exp(l_1 - l^*) + \\cdots + \\exp(l_n - l^*)\\bigg)\n\\]\nExample: \\(\\bf{\\rho = 30, \\, \\sigma = 2, \\,d = 11}\\).\nThis particular set of parameters admits a relatively concise likelihood, which will help us to write the complete formulae, starting with the log-likelihood:\n\\[\nl(11 | 2,30) = -\\log \\textstyle  \\binom{62}{11} + \\log \\bigg(2^{11} \\binom{2}{0}\\binom{30}{11} + 2^{10} \\binom{2}{1}\\binom{30}{10} + 2^{9} \\binom{2}{2}\\binom{30}{9} \\bigg).\n\\]\nIf we do this by brute force, using R to evaluate each of the powers, and binomial coefficients this becomes:\n\n\n\n\\[\n\\begin{align}\n l(11|2,30) & = -\\log 508271323092 \\\\\n & \\qquad + \\log \\big ( \n  111876710400 + 61532190720 + 7325260800\n \\big) \\\\\n & = - 26.95 + 25.92 \\\\\n & = -1.034\n\\end{align}\\]\nAnd back on the natural scale, that implies a likelihood of \\(L(11 |2,30) \\approx 0.36\\).\nThe bit that feels uncomfortable there is the sum of all those +10 digit numbers - and in real cases we could be summing more terms - each with more digits. So now let’s see how that looks using the log-sum-exp trick.\nAs per the statement above, we’ll first calculate the log of each of the terms (and include the denominator term, denoted \\(A\\) here, which we’ll need for the full calculation):\n\n\n\n\\[\n\\begin{align}\n\\textstyle \\log \\binom{62}{11} & = A  \\approx 26.9543 \\\\\n\\textstyle \\log 2^{11} \\binom{2}{0}\\binom{30}{11} &= l_1 \\approx 25.4407 \\\\\n\\textstyle \\log  2^{10} \\binom{2}{1}\\binom{30}{10} &= l_2 \\approx 24.8428 \\\\\n\\textstyle \\log 2^{9} \\binom{2}{2}\\binom{30}{9} &= l_3 \\approx 22.7146 \\\\\n\\end{align}\n\\] The maximum is \\(l^* = l_1\\), and we can go ahead and apply the trick: \\[\n\\begin{align}\nl(11|2,30) & = - A + l^* + \\log \\big ( e^{l_1 - l^*} + e^{l_2 - l^*} + e^{l_3 - l^*}\\big) \\\\\n& = -1.514 + \\log \\big( e^{0} + e^{-0.5978} + e^{-2.726} \\big) \\\\\n& = -1.514 + \\log \\big( 1+ {0.55} + {0.06548} \\big) \\\\\n& = -1.514 + 0.4796 \\\\\n& =  -1.034\n\\end{align}\n\\]\nAs expected we get exactly the same result - with the benefit that the largest value we used in any of the sums was \\(A \\approx 26.95\\).\n\n\nProof Given \\(x_1,\\ldots,x_n > 0\\), let \\(l_k = \\log(x_k)\\) and \\(l^* = \\max_k l_k\\). Then\n\\[\n\\log(x_1 + \\cdots +  x_n) =  l^* + \\log \\bigg ( \\exp(l_1 - l^*) + \\cdots + \\exp(l_n - l^*)\\bigg)\n\\]\nProof\nThe trick itself follows some from some fairly routine manipulation of exponentials. In the below, we have \\(x_k,\\,l_k\\) defined as above, and let \\(A\\) be any constant:\n\\[\n\\begin{align*}\nx_1 + \\cdots + x_n & = \\exp( \\log x_1) + \\cdots + \\exp( \\log x_n) \\\\\n& = \\exp(l_1) + \\cdots + \\exp(l_n) \\\\\n& = \\exp(A) \\bigg(\\exp(l_1 - A) + \\cdots + \\exp (l_n - A) \\bigg)\n\\end{align*}\n\\] Taking logarithms of both sides\n\\[\\log(x_1 + \\cdots + x_n) = A + \\log \\bigg ( \\exp(l_1 - A) + \\cdots + \\exp(l_n - A)\\bigg),\\] and the trick follows by choosing \\(A = l^* = \\max_k l_k\\).\n\n\nImportantly subtracting the maximum guarantees that each of the terms to be exponentiated satisfies \\(l_k - l^* \\leq 0\\), and hence each exponential is bounded above by 1, avoiding any calculations at the limits of machine precision.\nBååth’s Prior\nRasmus took the approach of defining the prior distribution in terms of a distribution on the total number of socks and the proportion of these that are singletons.\nFor Rasmus this choice doesn’t create an issues, as its perfectly easy to sample sample from this prior, and hence to conduct ABC. However as we are looking to do exact Bayesian inference, we have to derive the explicit formula for the prior - and that isn’t trivial.\nIt is, however, doable and leads to the following formula:\n\\[ P(\\rho, \\sigma) = P_{\\mu, \\tau}(2\\rho + \\sigma) \\left\\{ F_{\\alpha,\\beta}\\left( \\frac{2\\rho + 1}{2 \\lfloor \\rho + \\sigma/2\\rfloor}\\right) - F_{\\alpha,\\beta}\\left( \\frac{2\\rho - 1}{2 \\lfloor \\rho + \\sigma/2\\rfloor}\\right) \\right\\}\\]\nUtility Functions\n\nShow code\n# function to calculate grid of log posterior probabilities.\n#   - if prior_func = NULL returns the likelihood,\n#   - if likelihood_func = NULL returns the prior distribution\ncalculate_sock_grid <- function(rho_max,sigma_max, log_likelihood = NULL, log_prior = NULL){\n    \n  if(is.null(log_likelihood) & is.null(log_prior)){\n    stop(\"At least one of log_likelihood or log_prior must be provided\")\n  }\n\n  # initialise grid\n  grid <- crossing(\n      rho = 0:rho_max,\n      sigma = 0:sigma_max,\n      log_likelihood = NA_real_,\n      log_prior = NA_real_,\n      log_posterior = NA_real_\n    ) %>% rowwise()\n  \n\n  # populate prior/likelihood\n  if(!is.null(log_likelihood)){\n    grid <- grid %>% mutate(log_likelihood = log_likelihood(rho,sigma))\n  }\n  \n  if(!is.null(log_prior)){\n    grid <- grid %>% mutate(log_prior = log_prior(rho,sigma))\n  }\n  \n  # populate posterior, first calculating without the additive constant, Z, then\n  # evaluating this and adding.\n  grid <- grid %>% mutate(prop_log_posterior = log_likelihood + log_prior)\n  \n  log_Z <- logSumExp(grid$prop_log_posterior)\n    \n  grid <- grid %>% \n    mutate(log_posterior = -log_Z + prop_log_posterior) %>%\n    select(-prop_log_posterior)\n    \n  # calculate non-log terms\n  grid <- grid %>%\n    mutate(across(starts_with(\"log_\"), ~exp(.), .names = \"exp_{col}\")) %>%\n    rename_with(~str_remove(.,\"exp_log_\"))\n  \n  return(grid)\n}\n\nplot_sock_grid <- function(grid, var){\n  \n  var_name <- rlang::as_name(enquo(var))\n  \n  if(str_detect(var_name, \"log\")){\n    \n    filled_grid <- grid %>% filter(!is.infinite({{var}}))\n    empty_grid <- grid %>% filter(is.infinite({{var}}))\n  } else {\n    filled_grid <- grid %>% filter(({{var}} != 0))\n    empty_grid <- grid %>% filter(({{var}} == 0))\n  }\n  \n  p <- ggplot() +\n    geom_point(data =filled_grid,aes(x = rho, y = sigma, color ={{var}}), size =4) +\n    geom_point(data = empty_grid, aes(x = rho, y = sigma, ), color = \"grey\", shape = 1, size =4) +\n    scale_x_continuous(breaks = seq(0,max(grid$rho),by=10)) +\n    scale_y_continuous(breaks = seq(0,max(grid$sigma),by=10)) +\n    scale_color_gradientn(\n      colours = wesanderson::wes_palette(\"Zissou1\", 1000, type = \"continuous\"),\n      name = \"\") +\n    coord_fixed() + \n    labs(x = \"ρ\", y = \"σ\") +\n    theme(\n      axis.title.y = element_text(angle = 0, vjust = 0.5),\n      axis.line = element_blank(), legend.key.width = unit(1.4, \"cm\")\n    )\n \n return(p)\n}\n\n\n\n\n\n\n",
    "preview": "posts/2021-07-25-bromans-socks/distill-preview.png",
    "last_modified": "2021-07-02T15:39:29+01:00",
    "input_file": "bromans-socks.utf8.md"
  },
  {
    "path": "posts/2021-06-25-tdf-shape-21/",
    "title": "The Changing Shape of the Tour de France",
    "description": "On the eve of the Grand Départ of the 2021 Tour de France, we'll use animation to quickly draw some insights from the race's history.",
    "author": [],
    "date": "2021-06-25",
    "categories": [
      "Visualisation",
      "Cycling"
    ],
    "contents": "\n\nContents\nMaillot Jaune\nLanterne Rouge\nR Code\nAcknowledgments\n\n\n\n\n\nTour de France stage Start/Finishes, 1903-2021.\nEven if you’ve never tuned in to watch a stage of the Tour de France, likely you’re familiar with the symbolic Yellow Jersey, aware of the super-human strength required to cycle thousands of kilometers over the course of a month, and the unfortunate lengths people will go to to acquire that strength.\nWhilst these symbols have been a part of the race since the early days, the route over which the drama plays out is ever changing. I created the animation above to get a better understanding of the shape of the Tour de France.\n\nThe Yellow Jersey was added in 1919, but cheating was an issue from the second edtion: the four top placed riders were all later disqualified.\nIn the sections below I’ll summarise what I like about the visualisation and what I think could be improved.\nMaillot Jaune\nThe Maillot Jaune, Yellow Jersey, is famously worn by the rider in the lead of the race at the start of each stage, and finally awarded to the overall fastest rider when the race concludes in Paris.\nThe highlight of the visualisation for me is that it provides a compelling example of the benefits of animation for data story telling: enabling me to draw insights about the race that I’d have struggled to do from tables and static maps alone.\nHere’s a few points that I found interesting.\nThe early editions of the Tour had very few stages\nThe first and second editions of the Tour comprised just 6 stages, compared to today’s 21. Further digging into the data indicates the stage distances were however significantly longer than modern editions.\n\nThe longest stage in the 1903 Tour spanned 471km, compared to a maximum of 250km in 2021.\nA tour of the perimeter?\nThe Tour used to be largely about traversing the outer regions of France: between 1905 and 1938 the only detour away from the boarders seems to be the requisite annual visit to Paris.\nVariation was guaranteed for the riders every few years though, when the race organisers would change their preference for a clockwise or anti-clockwise route.\nThe Missing Years\nThe lack of animation in the periods 1915-1918 and 1940-46 are a reminder of the impact that the First and Second World Wars had on the routines and attractions that we take for granted.\nFurther reading led me to the interesting fact that a 1940 Tour was planned, with the intent that riders would be drafted from soldiers stationed in France.\n\nThere’s a lot more information on the Wikipedia page Tour de France during World War II.\nLate to the Party\nEven into the 1990s its possible to pick out by eye some département that were yet to host the start or finish of a Tour stage. Hosting a tour stage costs and it could well be that these areas of the country had different ideas on how local taxes should be spent.\nPierre Breteau created an app for Le Monde that allows you to explore the frequency that the Tour has passed through each department. With the exception of Corsica (that was finally visited in 2013) and overseas territories, his data indicates that the final department to be visited was Indre in 1992.\nLanterne Rouge\nThe Lanterne Rouge, Red Lantern, is the dubious award given to the last placed rider in the Tour. Somewhat ironically its a title riders fight for as it comes with an offer to ride (and hence appearance fees) in the post-Tour Criterium races.\nThe animation is far from perfect, and could do with some tweaks and improvements - some of which I’ve listed below. Maybe I’ll get around to improving these ahead of Copenhagen 2022!\nStraight Lines, and Pedalo Races\nEvident from the very first frame of the animation is the simplification to straight line paths between stage start and end points. This is particularly stark when the route follows the coast, occasionally suggesting that the riders took to the water to complete the stage.\nA search for Tour de France shapefiles suggests that these don’t exist for public consumption, but detailed road books are published for recent editions of the race - so in theory perhaps more accurate paths could be traced from this data.\nTour de… Belgique?\nWhilst it took until 1992 for the Tour to visit every mainland department, the race first left France in 1947 with detours to both Brussels, and Luxembourg City.\nSince then the tour has made regular detours to nearby countries: for now I’ve removed these overseas excursions for want of a suitable way to incorporate them into the visualisation - but this feels like an easy fix for the future.\n5 of the last 10 Grand Departs have taken place elsewhere in Europe.\nImplausible Locations\nI’ve used geocoding to infer longitude/latitude coordinates from the start/finish location names, using the R tidygeocoder package. This isn’t always accurate, returning for instance Cherbourg, Australia instead of Cherbourg, France, or suggesting the 2010 Tour started at Rue de Rotterdam, Tours rather than Rotterdam in the Netherlands.\nThis leads to some pretty obvious errors in the animation, with implausible distances covered in a stage. An immediate data quality check that could be built in is to calculate the straight line distance between start/end points of a stage and flag if this is greater than the stage distance.\nR Code\n\n\n\n\nShow code\nlibrary(infreqthemes)\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(gganimate)\nlibrary(tdfData) # install.github(\"odaniel1/tdfData\")\n\n## ---- prepare stage data --------------------------------------------------------------\n\n# TdF stage data was scraped from Wikipedia; it has some limitations (discussed above).\n# Its available in the tdfData package from my GitHub account\ntdf_stages <-tdf_stages %>% clean_names()\n\n# the TdF did not take place during WWI and WWII; create 'empty' stages so that\n# the animation marks these years.\nmissing_years <- crossing(year = c(1915:1918, 1940:1946), stage = as.character(1:10)) %>%\n  mutate(date = as.Date(paste0(year, \"-07-\",stage), '%Y-%m-%d'))\n\n# add missing years to the data; the stage_no column defines the order for the animation.\ntdf_stages <- bind_rows(tdf_stages, missing_years) %>%\n  arrange(date) %>%\n  mutate(stage_no=1:n())\n\n## ---- prepare plot data ---------------------------------------------------------------\n\n# The approach to getting lines to fade in gganimate is adapted from:\n# stackoverflow.com/questions/58271332/gganimate-plot-where-points-stay-and-line-fades\nplot_data <- tdf_stages %>%\n  uncount(nrow(tdf_stages), .id = \"frame\") %>%\n  filter(stage_no <= frame) %>%\n  arrange(frame, stage_no) %>%\n  group_by(frame) %>%\n  mutate(\n    # which edition of the TdF does the current frame visualise?\n    frame_year = max(year),\n    # how many stages ago was the stage first visualised created?\n    tail = last(stage_no) - stage_no,\n    # transparency set to be 1 (full) for first visualiation, and then fade.\n    # size set to be larger within same edition of TdF, and then shrink.\n    point_alpha = if_else(tail == 0, 1, 0.7),\n    point_size = if_else(year == frame_year, 2, 1),\n    # Fade lines over 20 stages, and remove once a new edition of the TdF starts.\n    segment_alpha = pmax(0, (20-tail)/20) * (year == frame_year)\n  ) %>%\n  ungroup()\n\n\n## ---- create plot ---------------------------------------------------------------------\n\n# raw (non-animated) plot\np <- ggplot(plot_data) +\n  \n  # used to dynamically add the edition of the TdF\n  geom_text(\n    data=plot_data, aes(-4, 50.5, label = frame_year),\n    size = 12, hjust = 0, color =infreq_palette[\"darkblue\"]\n  ) +\n  \n  # background map of france (coord_map fixes the aspect ratio)\n  geom_polygon(\n    data = map_data(\"france\"), aes(x=long, y = lat, group = group),\n    fill = \"#f3be02\", color = \"#f6f4e6\", size = 0.1\n  ) +\n  coord_map() +\n  \n  # stage start and finish locations as points\n  geom_point(\n    aes(x=start_longitude,y=start_latitude, alpha = point_alpha),\n    size = plot_data$point_size\n  ) +\n  geom_point(\n    aes(x=finish_longitude,y=finish_latitude,group=stage_no, alpha = point_alpha),\n    size = plot_data$point_size\n  ) +\n  \n  # line segment connecting stage start/finish points.\n  geom_segment(\n    aes(\n      x=start_longitude,\n      xend=finish_longitude,\n      y=start_latitude,\n      yend=finish_latitude,\n      alpha = segment_alpha\n    ), color = infreq_palette[\"orange\"]\n  ) +\n  \n  # set themes and legend\n  scale_alpha(range = c(0,1)) +\n  guides(alpha = \"none\") +\n  theme_void() +\n  transition_manual(frame)\n\n# create animation; speed is set to display 10 stages per second; completes in ~4mins.\nanim_stages <- animate(p,\n  nframes = max(plot_data$frame),\n  duration = max(tdf_stages$stage_no) * 0.1)\n\n\n\n\n\n\nAcknowledgments\nI first became interested in visualising how the Tour de France had changed over time after coming across Pierre Breteau’s visualisation in Le Monde, which shows the frequency with which each département of France has been visited by the race.\nTo create the dataset I’m indebted to the contributors to Wikipedia for their efforts to tabulate all of the stages in the Tour’s history.\nTo obtain and visualise the data I relied heavily on R, and and in particular the work of the creators of the rvest, gganimate, and tidygeocoder packages.\n\n\n\n",
    "preview": "posts/2021-06-25-tdf-shape-21/img/tdf_shape_1915.gif",
    "last_modified": "2021-06-26T18:39:07+01:00",
    "input_file": {}
  }
]
